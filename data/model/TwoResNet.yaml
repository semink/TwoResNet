# This NONPARAMS sections are not recorded as hyperparameters in tensorboard
NONPARAMS:
  LOG:
    save_dir: experiments
    log_graph: True
    name: TwoResNet-paper-pure

  SUMMARY:
    max_depth: 4

  TRAINER:
    gpus: [0] # set 0 (not [0]) if you don't have GPU
    max_epochs: 50

  DATA:
    num_workers: 16
    pre_determined_cluster: null

  EARLY_STOPPING:
    patience: 20

  TEST:
    checkpoint: 
      la: 
        dir_path: path/to/checkpoint/
        file_name: best.ckpt
      bay:
        dir_path: path/to/checkpoint/
        file_name: best.ckpt
    
    on_time:
      # - # morning peak
      #   - '06:00'
      #   - '10:00'
      # - # evening peak
      #   - '14:00'
      #   - '20:00'


  METRIC:
    monitor_metric_name: 'validation'
    training_metric_name: 'training'

# This HPARAMS sections are recorded as hyperparameters in tensorboard
HPARAMS:
  DATA:
    batch_size: 32
    cluster_info:
      K: 5
      alpha: 0.5
    day_of_week: true
    horizon: 12
    time_feat_mode: sinusoidal
  
  HighResNet:
    max_diffusion_step: 2
    num_rnn_layers: 2
    rnn_units: 64
    seq_len: 12
    dropout: 0
  
  LowResNet:
    num_rnn_layers: 1
    rnn_units: 64
    seq_len: 12
    dropout: 0
  
  TEACHER_FORCING:
    mode:
      LowResNet: off
      HighResNet: on
    probability:
      half_life: 13.33 
      reduce_rate: 0.1425525  # teacher forcing probability drops this rate (per epoch) at the half life epoch
    

  OPTIMIZER:
    adam:
      lr: 0.003
      weight_decay: 0
      eps: 1.0e-3
    multisteplr:
      milestone_start_p: 0.03 # when the teacher forcing probability drops this number, learning rate starts descreasing
      milestone_steps: 5
      lr_reduced_by: 0.3
    LowResNet_loss_weight: 1
  
  TRAINER:
    gradient_clip_val: 5